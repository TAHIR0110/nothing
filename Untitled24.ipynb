{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1616dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in ./anaconda3/lib/python3.11/site-packages (0.5.3)\n",
      "Requirement already satisfied: jaxlib in ./anaconda3/lib/python3.11/site-packages (0.5.3)\n",
      "Requirement already satisfied: flax in ./anaconda3/lib/python3.11/site-packages (0.10.4)\n",
      "Requirement already satisfied: optax in ./anaconda3/lib/python3.11/site-packages (0.2.4)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in ./anaconda3/lib/python3.11/site-packages (from jax) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.25 in ./anaconda3/lib/python3.11/site-packages (from jax) (1.26.4)\n",
      "Requirement already satisfied: opt_einsum in ./anaconda3/lib/python3.11/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in ./anaconda3/lib/python3.11/site-packages (from jax) (1.15.2)\n",
      "Requirement already satisfied: msgpack in ./anaconda3/lib/python3.11/site-packages (from flax) (1.0.3)\n",
      "Requirement already satisfied: orbax-checkpoint in ./anaconda3/lib/python3.11/site-packages (from flax) (0.11.10)\n",
      "Requirement already satisfied: tensorstore in ./anaconda3/lib/python3.11/site-packages (from flax) (0.1.72)\n",
      "Requirement already satisfied: rich>=11.1 in ./anaconda3/lib/python3.11/site-packages (from flax) (13.7.1)\n",
      "Requirement already satisfied: typing_extensions>=4.2 in ./anaconda3/lib/python3.11/site-packages (from flax) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./anaconda3/lib/python3.11/site-packages (from flax) (6.0)\n",
      "Requirement already satisfied: treescope>=0.1.7 in ./anaconda3/lib/python3.11/site-packages (from flax) (0.1.9)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in ./anaconda3/lib/python3.11/site-packages (from optax) (2.0.0)\n",
      "Requirement already satisfied: chex>=0.1.87 in ./anaconda3/lib/python3.11/site-packages (from optax) (0.1.89)\n",
      "Requirement already satisfied: etils[epy] in ./anaconda3/lib/python3.11/site-packages (from optax) (1.12.2)\n",
      "Requirement already satisfied: toolz>=0.9.0 in ./anaconda3/lib/python3.11/site-packages (from chex>=0.1.87->optax) (0.12.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./anaconda3/lib/python3.11/site-packages (from rich>=11.1->flax) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.11/site-packages (from rich>=11.1->flax) (2.15.1)\n",
      "Requirement already satisfied: nest_asyncio in ./anaconda3/lib/python3.11/site-packages (from orbax-checkpoint->flax) (1.5.6)\n",
      "Requirement already satisfied: protobuf in ./anaconda3/lib/python3.11/site-packages (from orbax-checkpoint->flax) (4.25.6)\n",
      "Requirement already satisfied: humanize in ./anaconda3/lib/python3.11/site-packages (from orbax-checkpoint->flax) (4.12.1)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in ./anaconda3/lib/python3.11/site-packages (from orbax-checkpoint->flax) (3.20.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.0)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.11/site-packages (from etils[epy]->optax) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in ./anaconda3/lib/python3.11/site-packages (from etils[epy]->optax) (6.5.2)\n",
      "Requirement already satisfied: zipp in ./anaconda3/lib/python3.11/site-packages (from etils[epy]->optax) (3.11.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jax jaxlib flax optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2ee4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd521e",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a06f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_size)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f1473",
   "metadata": {},
   "source": [
    "# Create a Training State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea56952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, hidden_size, output_size):\n",
    "    model = SimpleNN(hidden_size=hidden_size, output_size=output_size)\n",
    "    params = model.init(rng, jnp.ones([1, 784]))['params']  # Example input shape\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89753b",
   "metadata": {},
   "source": [
    "# Define the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ecfe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(params, state, x, y):\n",
    "    preds = state.apply_fn({'params': params}, x)\n",
    "    return jnp.mean((preds - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f811d6",
   "metadata": {},
   "source": [
    "# Define the Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36f0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, x, y):\n",
    "    grad_fn = jax.grad(mse_loss)\n",
    "    grads = grad_fn(state.params, state, x, y)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85f27a",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f6e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(state, x_train, y_train, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        state = train_step(state, x_train, y_train)\n",
    "        if epoch % 10 == 0:\n",
    "            loss = mse_loss(state.params, state, x_train, y_train)\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9250326",
   "metadata": {},
   "source": [
    "# For just showing we'll use synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa61aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples, input_size, output_size):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(rng, (num_samples, input_size))\n",
    "    y = jnp.dot(x, jnp.ones((input_size, output_size))) + 0.1 * jax.random.normal(rng, (num_samples, output_size))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383454b6",
   "metadata": {},
   "source": [
    "#  Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343b542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 741.6810302734375\n",
      "Epoch 10, Loss: 104.13189697265625\n",
      "Epoch 20, Loss: 24.74224853515625\n",
      "Epoch 30, Loss: 11.690950393676758\n",
      "Epoch 40, Loss: 4.067071914672852\n",
      "Epoch 50, Loss: 1.7465698719024658\n",
      "Epoch 60, Loss: 0.7526335716247559\n",
      "Epoch 70, Loss: 0.39823049306869507\n",
      "Epoch 80, Loss: 0.22409191727638245\n",
      "Epoch 90, Loss: 0.14787310361862183\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    input_size = 784\n",
    "    hidden_size = 128\n",
    "    output_size = 10\n",
    "    num_samples = 1000\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    x_train, y_train = generate_data(num_samples, input_size, output_size)\n",
    "    state = create_train_state(rng, learning_rate, hidden_size, output_size)\n",
    "    state = train_model(state, x_train, y_train, num_epochs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7e38e",
   "metadata": {},
   "source": [
    "# Now adding \n",
    "Multiple hidden layers.\n",
    "Dropout for regularization.\n",
    "Batch normalization.\n",
    "Skip connections (residual blocks).\n",
    "A custom learning rate scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2f618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nh/_xrsx9nn1f91kstthd5v77940000gn/T/ipykernel_27710/2944867003.py:87: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  grads = jax.tree_map(lambda g: jnp.clip(g, -1.0, 1.0), grads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.108287811279297\n",
      "Epoch 10, Loss: 2.6931939125061035\n",
      "Epoch 20, Loss: 1.9992084503173828\n",
      "Epoch 30, Loss: 1.3193739652633667\n",
      "Epoch 40, Loss: 0.7016776204109192\n",
      "Epoch 50, Loss: 0.3615359961986542\n",
      "Epoch 60, Loss: 0.22701393067836761\n",
      "Epoch 70, Loss: 0.13619503378868103\n",
      "Epoch 80, Loss: 0.11714537441730499\n",
      "Early stopping at epoch 86\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "#residual block with dropout and batch normalization\n",
    "class ResidualBlock(nn.Module):\n",
    "    features: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        residual = x\n",
    "        x = nn.Dense(self.features)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
    "        x = nn.Dense(self.features)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        x += residual  \n",
    "        x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class TahirsComplexModel(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "    dropout_rate: float\n",
    "    num_blocks: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        #multiple residual blocks\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = ResidualBlock(features=self.hidden_size, dropout_rate=self.dropout_rate)(x, training)\n",
    "\n",
    "        x = nn.Dense(self.output_size)(x)\n",
    "        return x\n",
    "\n",
    "#learning rate scheduler\n",
    "def create_learning_rate_scheduler(base_learning_rate, warmup_epochs, total_epochs):\n",
    "    warmup_fn = optax.linear_schedule(\n",
    "        init_value=0.0, end_value=base_learning_rate, transition_steps=warmup_epochs\n",
    "    )\n",
    "    cosine_fn = optax.cosine_decay_schedule(\n",
    "        init_value=base_learning_rate, decay_steps=total_epochs - warmup_epochs\n",
    "    )\n",
    "    return optax.join_schedules(\n",
    "        schedules=[warmup_fn, cosine_fn], boundaries=[warmup_epochs]\n",
    "    )\n",
    "\n",
    "#Create a training state\n",
    "def create_train_state(rng, learning_rate, hidden_size, output_size, dropout_rate, num_blocks):\n",
    "    model = TahirsComplexModel(hidden_size=hidden_size, output_size=output_size, dropout_rate=dropout_rate, num_blocks=num_blocks)\n",
    "    rng1, rng2 = jax.random.split(rng)\n",
    "    variables = model.init({'params': rng1, 'dropout': rng2}, jnp.ones([1, 784]), training=True)\n",
    "    params = variables['params']\n",
    "    batch_stats = variables['batch_stats'] \n",
    "    lr_scheduler = create_learning_rate_scheduler(learning_rate, warmup_epochs=10, total_epochs=100)\n",
    "    tx = optax.adamw(lr_scheduler, weight_decay=1e-4)  \n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx\n",
    "    ), batch_stats\n",
    "\n",
    "#Define the loss function with L2 regularization\n",
    "def cross_entropy_loss(params, batch_stats, state, x, y, rng):\n",
    "    logits, new_batch_stats = state.apply_fn(\n",
    "        {'params': params, 'batch_stats': batch_stats},\n",
    "        x,\n",
    "        training=True,\n",
    "        rngs={'dropout': rng},\n",
    "        mutable=['batch_stats']  \n",
    "    )\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot_y * jax.nn.log_softmax(logits), axis=-1))\n",
    "    return loss, new_batch_stats\n",
    "\n",
    "#Define the training step with gradient clipping\n",
    "@jax.jit\n",
    "def train_step(state, batch_stats, x, y, rng):\n",
    "    grad_fn = jax.value_and_grad(cross_entropy_loss, has_aux=True)\n",
    "    (loss, new_batch_stats), grads = grad_fn(state.params, batch_stats, state, x, y, rng)\n",
    "    grads = jax.tree_map(lambda g: jnp.clip(g, -1.0, 1.0), grads)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, new_batch_stats, loss\n",
    "\n",
    "def train_model(state, batch_stats, x_train, y_train, num_epochs, rng):\n",
    "    best_loss = float('inf')\n",
    "    patience = 5  \n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        rng, dropout_rng = jax.random.split(rng)\n",
    "        state, batch_stats, loss = train_step(state, batch_stats, x_train, y_train, dropout_rng)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "    return state, batch_stats\n",
    "\n",
    "def generate_data(num_samples, input_size, output_size):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(rng, (num_samples, input_size))\n",
    "    y = jax.random.randint(rng, (num_samples,), 0, output_size)\n",
    "    return x, y\n",
    "\n",
    "def main():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    input_size = 784\n",
    "    hidden_size = 256\n",
    "    output_size = 10\n",
    "    num_samples = 1000\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    dropout_rate = 0.5\n",
    "    num_blocks = 5\n",
    "\n",
    "    x_train, y_train = generate_data(num_samples, input_size, output_size)\n",
    "    state, batch_stats = create_train_state(rng, learning_rate, hidden_size, output_size, dropout_rate, num_blocks)\n",
    "    state, batch_stats = train_model(state, batch_stats, x_train, y_train, num_epochs, rng)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfce1f9",
   "metadata": {},
   "source": [
    "# Now we have warm up and now we will implement and train a miniGPT-like model using JAX and Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax flax optax\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(\"shakespeare.txt\", \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"Downloaded shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"shakespeare.txt\", \"r\").read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "data = jnp.array([char_to_idx[ch] for ch in text])\n",
    "\n",
    "seq_length = 128\n",
    "inputs = [data[i:i+seq_length] for i in range(len(data) - seq_length)]\n",
    "targets = [data[i+1:i+seq_length+1] for i in range(len(data) - seq_length)]\n",
    "inputs = jnp.stack(inputs)\n",
    "targets = jnp.stack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a739c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        # Multi-head self-attention\n",
    "        attn_output = nn.MultiHeadDotProductAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            qkv_features=self.embed_dim,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            deterministic=not training,\n",
    "        )(x, x)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = x + nn.Dropout(rate=self.dropout_rate, deterministic=not training)(attn_output)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Feedforward network\n",
    "        ff_output = nn.Dense(self.embed_dim * 4)(x)\n",
    "        ff_output = nn.relu(ff_output)\n",
    "        ff_output = nn.Dense(self.embed_dim)(ff_output)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = x + nn.Dropout(rate=self.dropout_rate, deterministic=not training)(ff_output)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool):\n",
    "        # Token embeddings\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])[None, :]\n",
    "        pos_embeddings = nn.Embed(seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings\n",
    "\n",
    "        # Transformer blocks\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerBlock(\n",
    "                embed_dim=self.embed_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                dropout_rate=self.dropout_rate,\n",
    "            )(x, training)\n",
    "\n",
    "        # Output logits\n",
    "        logits = nn.Dense(self.vocab_size)(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4154976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = MiniGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = model.init(rng, jnp.ones((1, seq_length), dtype=jnp.int32), training=False)['params']\n",
    "\n",
    "# Define the loss function\n",
    "def cross_entropy_loss(params, inputs, targets):\n",
    "    logits = model.apply({'params': params}, inputs, training=True)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "    return loss\n",
    "\n",
    "# Create the optimizer\n",
    "learning_rate = 0.001\n",
    "tx = optax.adamw(learning_rate)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=tx,\n",
    ")\n",
    "\n",
    "# Training step\n",
    "@jax.jit\n",
    "def train_step(state, inputs, targets):\n",
    "    grad_fn = jax.grad(cross_entropy_loss)\n",
    "    grads = grad_fn(state.params, inputs, targets)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "# Training loop\n",
    "batch_size = 64\n",
    "for epoch in range(10):  # Train for 10 epochs\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch_inputs = inputs[i:i+batch_size]\n",
    "        batch_targets = targets[i:i+batch_size]\n",
    "        state = train_step(state, batch_inputs, batch_targets)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {cross_entropy_loss(state.params, batch_inputs, batch_targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5590cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(state, start_string, num_chars=100):\n",
    "    input_ids = jnp.array([char_to_idx[ch] for ch in start_string])\n",
    "    for _ in range(num_chars):\n",
    "        logits = model.apply({'params': state.params}, input_ids[None, :], training=False)\n",
    "        next_id = jnp.argmax(logits[0, -1])\n",
    "        input_ids = jnp.append(input_ids, next_id)\n",
    "    return ''.join([idx_to_char[i] for i in input_ids])\n",
    "\n",
    "# Generate text\n",
    "start_string = \"ROMEO:\"\n",
    "generated_text = generate_text(state, start_string, num_chars=200)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
